{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center Plant Disease Detection Using CNN</center>\n",
        "\n"
      ],
      "metadata": {
        "id": "ObIn1BAwDbYS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E_nZtVWzdQZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0911a21a-201b-4f42-bb3c-6d1d23fc79c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PlantVillage-Dataset'...\n",
            "remote: Enumerating objects: 163235, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 163235 (delta 2), reused 1 (delta 0), pack-reused 163229 (from 1)\u001b[K\n",
            "Receiving objects: 100% (163235/163235), 2.00 GiB | 31.09 MiB/s, done.\n",
            "Resolving deltas: 100% (101/101), done.\n",
            "Updating files: 100% (182401/182401), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/spMohanty/PlantVillage-Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **PlantVillage-Dataset** from GitHub, This dataset contains images of various plant diseases. **54K+ images**\n",
        "\n",
        "Set the path to the root directory where the raw color images are stored."
      ],
      "metadata": {
        "id": "-yGO5xQ3Cx63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_ROOT = \"/content/PlantVillage-Dataset/raw/color\""
      ],
      "metadata": {
        "id": "Iu9FXoTAgRVQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List the contents of the dataset directory to verify the download.\n",
        "\n",
        "`head` shows only the first few folders (the classes)"
      ],
      "metadata": {
        "id": "INjlBgSHvuXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls $DATASET_ROOT | head"
      ],
      "metadata": {
        "id": "DBGwyVtLhxjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "379f7085-99af-430a-b572-b3b98531ab09"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple___Apple_scab\n",
            "Apple___Black_rot\n",
            "Apple___Cedar_apple_rust\n",
            "Apple___healthy\n",
            "Blueberry___healthy\n",
            "Cherry_(including_sour)___healthy\n",
            "Cherry_(including_sour)___Powdery_mildew\n",
            "Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\n",
            "Corn_(maize)___Common_rust_\n",
            "Corn_(maize)___healthy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the necessary Python libraries for deep learning and data handling.\n",
        "\n",
        "This ensures all dependencies are met for the rest of the code"
      ],
      "metadata": {
        "id": "v2ulPFBVEGZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tensorflow tensorflow-datasets matplotlib seaborn scikit-learn"
      ],
      "metadata": {
        "id": "jyiLdiAphTJX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# Import specific modules from Keras for building the neural network.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "2CyYDM6Mccqk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_size = (224, 224)\n",
        "batch_size = 16\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    validation_split=0.2,\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")"
      ],
      "metadata": {
        "id": "kfr6sRjGhZ01"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = datagen.flow_from_directory(\n",
        "    DATASET_ROOT,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    subset=\"training\",\n",
        "    class_mode=\"categorical\"\n",
        ")"
      ],
      "metadata": {
        "id": "vUfX9KdLuH_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cabde0a-72e6-4f48-cf58-e348a7c1970f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 43456 images belonging to 38 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_gen = datagen.flow_from_directory(\n",
        "    DATASET_ROOT,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    subset=\"validation\",\n",
        "    class_mode=\"categorical\"\n",
        ")"
      ],
      "metadata": {
        "id": "MuyzN0s6uKQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f08e855-93ff-492b-91ae-39493cd2e321"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10849 images belonging to 38 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(train_gen.class_indices)\n",
        "print(\"Classes:\", train_gen.class_indices)"
      ],
      "metadata": {
        "id": "jYv23MWfhjz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2031566b-3059-4320-ff83-63ac94d0fe4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: {'Apple___Apple_scab': 0, 'Apple___Black_rot': 1, 'Apple___Cedar_apple_rust': 2, 'Apple___healthy': 3, 'Blueberry___healthy': 4, 'Cherry_(including_sour)___Powdery_mildew': 5, 'Cherry_(including_sour)___healthy': 6, 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': 7, 'Corn_(maize)___Common_rust_': 8, 'Corn_(maize)___Northern_Leaf_Blight': 9, 'Corn_(maize)___healthy': 10, 'Grape___Black_rot': 11, 'Grape___Esca_(Black_Measles)': 12, 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': 13, 'Grape___healthy': 14, 'Orange___Haunglongbing_(Citrus_greening)': 15, 'Peach___Bacterial_spot': 16, 'Peach___healthy': 17, 'Pepper,_bell___Bacterial_spot': 18, 'Pepper,_bell___healthy': 19, 'Potato___Early_blight': 20, 'Potato___Late_blight': 21, 'Potato___healthy': 22, 'Raspberry___healthy': 23, 'Soybean___healthy': 24, 'Squash___Powdery_mildew': 25, 'Strawberry___Leaf_scorch': 26, 'Strawberry___healthy': 27, 'Tomato___Bacterial_spot': 28, 'Tomato___Early_blight': 29, 'Tomato___Late_blight': 30, 'Tomato___Leaf_Mold': 31, 'Tomato___Septoria_leaf_spot': 32, 'Tomato___Spider_mites Two-spotted_spider_mite': 33, 'Tomato___Target_Spot': 34, 'Tomato___Tomato_Yellow_Leaf_Curl_Virus': 35, 'Tomato___Tomato_mosaic_virus': 36, 'Tomato___healthy': 37}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Model (Transfer Learning)"
      ],
      "metadata": {
        "id": "L72tyLFiibF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a pre-trained EfficientNetB0 model.\n",
        "\n",
        "`include_top=False` means we are only using the feature-extracting base,not the final classification layers.\n",
        "\n",
        "Freeze the base model's layers so their weights are not updated during training."
      ],
      "metadata": {
        "id": "Y1UY0Hr_FKbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.EfficientNetB0(\n",
        "    input_shape=(224,224,3),\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\"\n",
        ")\n",
        "base_model.trainable = False  # Freeze base initially\n"
      ],
      "metadata": {
        "id": "t-4gDJO2z-7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d13fee-7085-4fc9-f869-4ac0383239dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a GlobalAveragePooling2D layer on top of the base model's output.\n",
        "\n",
        "This averages the feature maps, preparing the data for the final classification layers."
      ],
      "metadata": {
        "id": "nSiIhJ6A0GWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)"
      ],
      "metadata": {
        "id": "-K_XXRv1-PZY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a BatchNormalization layer to stabilize and speed up training."
      ],
      "metadata": {
        "id": "5QNutI5lFvZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Normalisation\n",
        "x = tf.keras.layers.BatchNormalization()(x)"
      ],
      "metadata": {
        "id": "Rx5dkQJX-Cgp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a Dropout layer to randomly ignore some neurons, which helps prevent overfitting."
      ],
      "metadata": {
        "id": "ozFw7mHWFz5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout for regularization\n",
        "x = tf.keras.layers.Dropout(0.4)(x)"
      ],
      "metadata": {
        "id": "fisfqhra0WpU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a fully connected (Dense) layer with 256 neurons and ReLU activation."
      ],
      "metadata": {
        "id": "LWF3kiWtF7fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.keras.layers.Dense(256, activation=\"relu\")(x)"
      ],
      "metadata": {
        "id": "J0oyR-Af-RD4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add another BatchNormalization layer."
      ],
      "metadata": {
        "id": "EdvZdk_SbHh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Normalisation after dense layer\n",
        "x = tf.keras.layers.BatchNormalization()(x)"
      ],
      "metadata": {
        "id": "FcSXVerOJlDv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.keras.layers.Dropout(0.4)(x)\n",
        "outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)"
      ],
      "metadata": {
        "id": "f8YLa65u4Jnq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new Sequential model, which stacks layers one after another, Add convolutional layers and pooling layers to extract features from images, more complex features.\n",
        "\n",
        "Flatten the output of the convolutional layers into a 1D vector, Add a fully connected layer with 128 neurons\n",
        "\n",
        "\n",
        "Add the final output layer with `num_classes` neurons for classification."
      ],
      "metadata": {
        "id": "xfznRYSmbQ3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Dropout(0.2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Dropout(0.2),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "DypDswjw4P07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18a2437-9408-4c71-fb64-6fc37e610e80"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the Model to configure the training,\n",
        "\n",
        "Using the Adam optimizer and categorical_crossentropy as loss function for multiclass classification."
      ],
      "metadata": {
        "id": "xsHzn7Oa4UkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "__2mQPn44T-4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation (Second, more detailed attempt)\n",
        "\n",
        "Create a new ImageDataGenerator for training with a wider range of augmentations."
      ],
      "metadata": {
        "id": "WZJAXjvVb3Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "Z6ReO_o2um6g"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the Dataset into train and val and creating separate folders for both, for training model images from train folder is used"
      ],
      "metadata": {
        "id": "Sj115IsoN4dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Create train/val directories\n",
        "train_dir = \"/content/PlantVillage-Dataset/train\"\n",
        "val_dir = \"/content/PlantVillage-Dataset/val\"\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "j5POp_2q1KDN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loop through each class folder to divide the dataset to 80:20 train val"
      ],
      "metadata": {
        "id": "Sips618TOQLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = \"/content/PlantVillage-Dataset/raw/color\"\n",
        "for class_name in os.listdir(dataset_dir):\n",
        "    class_path = os.path.join(dataset_dir, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        images = os.listdir(class_path)\n",
        "        random.shuffle(images)\n",
        "        split = int(len(images) * 0.8)  # 80% train, 20% val\n",
        "\n",
        "        train_class_dir = os.path.join(train_dir, class_name)\n",
        "        val_class_dir = os.path.join(val_dir, class_name)\n",
        "        os.makedirs(train_class_dir, exist_ok=True)\n",
        "        os.makedirs(val_class_dir, exist_ok=True)\n",
        "\n",
        "        # Move images\n",
        "        for img in images[:split]:\n",
        "            shutil.copy(os.path.join(class_path, img), train_class_dir)\n",
        "        for img in images[split:]:\n",
        "            shutil.copy(os.path.join(class_path, img), val_class_dir)"
      ],
      "metadata": {
        "id": "crV3yYP-1lnB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a new generator for the training data using the new `train` directory"
      ],
      "metadata": {
        "id": "qKzgBLWKOfd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/PlantVillage-Dataset/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "id": "j3YMu9MGuq1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0dda509-d02f-4174-d365-fd725ed6a045"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 43429 images belonging to 38 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a separate generator for the validation data."
      ],
      "metadata": {
        "id": "uh3UWst6Okdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_generator = test_datagen.flow_from_directory(\n",
        "    '/content/PlantVillage-Dataset/val',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "id": "uckFTZQW2Aw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4de5d0-a1d6-4ed9-9bc2-201b7c5d0937"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10876 images belonging to 38 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model training"
      ],
      "metadata": {
        "id": "59LCbSrGOtZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=30,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // val_generator.batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "xQ3s2g7lumxF",
        "outputId": "b13fe3b9-98ea-47bd-f5bc-ec1549036554",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 400ms/step - accuracy: 0.2373 - loss: 3.2506 - val_accuracy: 0.4828 - val_loss: 1.8182\n",
            "Epoch 2/50\n",
            "\u001b[1m   1/1357\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 65ms/step - accuracy: 0.3750 - loss: 2.1823"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.3750 - loss: 2.1823 - val_accuracy: 0.4950 - val_loss: 1.7652\n",
            "Epoch 3/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 416ms/step - accuracy: 0.4505 - loss: 1.9165 - val_accuracy: 0.6396 - val_loss: 1.1960\n",
            "Epoch 4/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.4688 - loss: 1.4112 - val_accuracy: 0.6218 - val_loss: 1.2496\n",
            "Epoch 5/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m600s\u001b[0m 410ms/step - accuracy: 0.5340 - loss: 1.5857 - val_accuracy: 0.6950 - val_loss: 1.0149\n",
            "Epoch 6/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - accuracy: 0.4375 - loss: 1.5220 - val_accuracy: 0.7011 - val_loss: 0.9940\n",
            "Epoch 7/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m564s\u001b[0m 416ms/step - accuracy: 0.5740 - loss: 1.4290 - val_accuracy: 0.7215 - val_loss: 0.9255\n",
            "Epoch 8/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - accuracy: 0.6875 - loss: 1.0596 - val_accuracy: 0.7136 - val_loss: 0.9453\n",
            "Epoch 9/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 396ms/step - accuracy: 0.6052 - loss: 1.3105 - val_accuracy: 0.7503 - val_loss: 0.7919\n",
            "Epoch 10/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.5938 - loss: 1.3421 - val_accuracy: 0.7578 - val_loss: 0.7772\n",
            "Epoch 11/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 407ms/step - accuracy: 0.6259 - loss: 1.2439 - val_accuracy: 0.7181 - val_loss: 0.9239\n",
            "Epoch 12/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.5000 - loss: 1.6523 - val_accuracy: 0.7111 - val_loss: 0.9480\n",
            "Epoch 13/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 387ms/step - accuracy: 0.6499 - loss: 1.1493 - val_accuracy: 0.7659 - val_loss: 0.7390\n",
            "Epoch 14/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.7500 - loss: 0.6340 - val_accuracy: 0.7657 - val_loss: 0.7382\n",
            "Epoch 15/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 398ms/step - accuracy: 0.6642 - loss: 1.1077 - val_accuracy: 0.7187 - val_loss: 0.9362\n",
            "Epoch 16/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - accuracy: 0.6250 - loss: 1.1720 - val_accuracy: 0.7140 - val_loss: 0.9634\n",
            "Epoch 17/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m560s\u001b[0m 409ms/step - accuracy: 0.6757 - loss: 1.0622 - val_accuracy: 0.7322 - val_loss: 0.8792\n",
            "Epoch 18/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 15ms/step - accuracy: 0.7188 - loss: 0.8543 - val_accuracy: 0.7366 - val_loss: 0.8671\n",
            "Epoch 19/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m586s\u001b[0m 397ms/step - accuracy: 0.6855 - loss: 1.0282 - val_accuracy: 0.7754 - val_loss: 0.6957\n",
            "Epoch 20/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.8438 - loss: 0.6360 - val_accuracy: 0.7804 - val_loss: 0.6789\n",
            "Epoch 21/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 393ms/step - accuracy: 0.6962 - loss: 0.9849 - val_accuracy: 0.7847 - val_loss: 0.7214\n",
            "Epoch 22/50\n",
            "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.6250 - loss: 1.1668 - val_accuracy: 0.7840 - val_loss: 0.7266\n",
            "Epoch 23/50\n",
            "\u001b[1m1342/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 383ms/step - accuracy: 0.7101 - loss: 0.9487"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "di5G2PLI4ZU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aimbJQqZPckh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mwh_QrlJUpSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained Keras model\n",
        "model.save('/content/crop_disease_model.h5')\n",
        "print(\"Model saved as crop_disease_model.h5\")"
      ],
      "metadata": {
        "id": "RT-j7NMWYG-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "l1IUiFi2Zp8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder inside Google Drive\n",
        "save_path = \"/content/drive/MyDrive/CropDiseaseModel\"\n",
        "import os\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save Keras model (.h5)\n",
        "model.save(f\"{save_path}/crop_disease_model.h5\")\n",
        "\n",
        "# Save TFLite model (.tflite)\n",
        "import tensorflow as tf\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "with open(f\"{save_path}/crop_disease_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"✅ Model saved to Google Drive at:\", save_path)\n"
      ],
      "metadata": {
        "id": "vsiL5ts9aK5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "62KNlV3FaLbm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}